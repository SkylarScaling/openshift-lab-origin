= Deploying Clusters using HyperShift

== Introduction

This Red Hat Product Demo System catalog item allows you to use, demonstrate and learn *Hypershift* - also known as *Hosted Control Planes*.

[TIP]
====
If you received a link to this document and need an environment then go to the https://demo.redhat.com[RedHat Product Demo System (Beta)] and find the catalog item *Red Hat Advanced Cluster Management with HyperShift* in the *Demos* folder. You can also type *hypershift* into the search box to find the catalog item.

Make sure you provision an environment before continuing. Provisioning should take approximately an hour.
====

Hypershift is a way to deploy the Control Plane of Red Hat OpenShift Container Platform clusters as pods on other clusters dramatically decreasing the time it takes to provision a new cluster. Worker nodes are still deployed as virtual machines and managed using the new `Machine.cluster.x-k8s.io/v1beta1` API.

Hypershift is currently in Tech Preview on both Red Hat Advanced Cluster Management for Kubernetes versions 2.5 and 2.6.

The catalog item deploys the following environment::

* OpenShift Container Platform 4.11.
* Red Hat Advanced Cluster Management for Kubernetes (RHACM) 2.6 installed and configured.
* Technology Preview for HyperShift enabled and configured on RHACM.
* A small cluster has been pre-deployed using HyperShift.

The instructions cover the following scenarios:

* Explore the pre-installed environment (this document)
* https://github.com/redhat-cop/openshift-lab-origin/blob/master/HyperShift/Deploy_Cluster.adoc[Deploy a cluster using HyperShift].
* https://github.com/redhat-cop/openshift-lab-origin/blob/master/HyperShift/Deploy_Application.adoc[Deploy an application to both clusters using RHACM].
* https://github.com/redhat-cop/openshift-lab-origin/blob/master/HyperShift/Hypershift_Setup.adoc[Background on how HyperShift was installed and configured on the environment].

== Documentation

Further documentation on Hypershift can be found at the following links:

* RHACM 2.5: https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.5/html/clusters/managing-your-clusters#hosted-control-plane-intro
* RHACM 2.6: https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html/multicluster_engine/multicluster_engine_overview#hosted-control-planes-intro

== Information you will need

Once your environment has been provisioned you will receive an e-mail with the following information:

[source,text]
----
Openshift Console: https://console-openshift-console.apps.cluster-rrjmf.sandbox2589.opentlc.com

Openshift API for command line 'oc' client: https://api.cluster-rrjmf.sandbox2589.opentlc.com:6443

Download oc client from http://d3s3zqyaz8cp2d.cloudfront.net/pub/openshift-v4/clients/ocp/stable-4.11/openshift-client-linux.tar.gz

HTPasswd Authentication is enabled on this cluster.

Users user1 .. user2 are created with password openshift

User admin with password r3dh4t1! is cluster admin.

You can access Gitea via the URL https://gitea.apps.cluster-rrjmf.sandbox2589.opentlc.com

The Gitea admin username is 'opentlc-mgr'.

The Gitea admin password is 'openshift'.

Gitea users were created, from user1 to user2 with the password 'openshift'

Your RHACM console is available at:
https://multicloud-console.apps.cluster-rrjmf.sandbox2589.opentlc.com

You can access your bastion via SSH: ssh lab-user@bastion.rrjmf.sandbox2589.opentlc.com

Make sure you use the username 'lab-user' and the password '2gQ8Lmqpdg56' when prompted.
----

Make note of the following information:

* The SSH command to connect to you environment's bastion's VM
* The password to be used with user `lab-user` on the bastion VM
* Also the OpenShift Console URL along with the credentials for the `admin` user.

There is plenty more information below what is illustrated above. But all of those properties have already been set up in your environment.

== Explore environment

Let's explore what has been set up in the environment.

. Log into the bastion VM using the provided SSH command and password.
+
.Example (use your own URL)
[source,sh]
----
ssh lab-user@bastion.rrjmf.sandbox2589.opentlc.com
----

. On the bastion VM examine the hub cluster:
+
[source,sh]
----
oc get nodes
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
NAME                                         STATUS   ROLES    AGE     VERSION
ip-10-0-131-232.us-east-2.compute.internal   Ready    worker   3h55m   v1.24.0+dc5a2fd
ip-10-0-132-249.us-east-2.compute.internal   Ready    master   4h2m    v1.24.0+dc5a2fd
ip-10-0-209-203.us-east-2.compute.internal   Ready    worker   3h55m   v1.24.0+dc5a2fd
ip-10-0-215-237.us-east-2.compute.internal   Ready    worker   3h55m   v1.24.0+dc5a2fd
ip-10-0-251-50.us-east-2.compute.internal    Ready    master   4h3m    v1.24.0+dc5a2fd
ip-10-0-254-250.us-east-2.compute.internal   Ready    master   4h3m    v1.24.0+dc5a2fd
----

. You will see that your hub cluster has three control plane nodes (`master`) and three worker nodes (`worker`).

. Now switch to the project `hypershift`
+
[source,sh]
----
oc project hypershift
----

. This is the project that has been set up to hold all Hypershift related resources.
+
Get a list of deployed Hypershift environments:
+
[source,sh]
----
oc get hypershiftdeployments
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
NAME          TYPE   INFRA                  IAM                    MANIFESTWORK           PROVIDER REF   PROGRESS    AVAILABLE
development   AWS    ConfiguredAsExpected   ConfiguredAsExpected   ConfiguredAsExpected   AsExpected     Completed   True
----
+
[NOTE]
====
If your environment just finished provisioning you may see that the Hypershift Deployment is still progressing. In that case the output will look something like this:

[source,texinfo,options=nowrap]
----
NAMESPACE    NAME          TYPE   INFRA                  IAM                    MANIFESTWORK           PROVIDER REF   PROGRESS   AVAILABLE
hypershift   development   AWS    ConfiguredAsExpected   ConfiguredAsExpected   ConfiguredAsExpected   AsExpected     Partial    True
----

In that case wait until the Hypershift Deployment shows that Progress is *Completed*.
====

. Examine the Hypershift Deployment:
+
[source,sh]
----
oc describe hd development
----
+
You will see *a lot* of information about this cluster. Note the following items:

* *Etcd*: You will see that the ETCD storage is backed by a `PersistentVolumeClaim` on the hub cluster.
* *Networking*: You will see the various `CIDR` blocks for IP addresses for the cluster. You will also see that the network type for this environment has been set to `OVNKubernetes`.
* *Platform/AWS*: Properties about the hosting platform (only AWS is supported in the Tech Preview). You will see that your Hypershift cluster has been deployed in the `us-west-2a` zone in the region `us-west-2`. Note that this is only true for worker nodes and networking infrastructure like Elastic Load Balancers. The control plane is entirely running as pods on the hub cluster. We will explore that a little bit later.
* *Release/Image*: The OpenShift release that was installed.
* *Services*: which components of OpenShift have been set up with which properties.
* *Hosting Cluster*: which cluster is hosting the control plane for this cluster. In our case this is the `local-cluster` - also known as the hub cluster.
* *Infrastructure*: More details about the (AWS) infrastructure that was used.
* *Node Pools*: these are the pools of worker nodes that have been provisioned. For this environment only one worker pool named `development-us-west-2a` has been created with 2 worker node replicas. Again there is a release image specified for the worker nodes.
+
In the next lab you will set up another, production, cluster that will have some different properties - and two node pools.

. Switch to the project `clusters-development` which holds all the resources for this `development` cluster.
+
[source,sh]
----
oc project clusters-development
----

. List the pods in this project:
+
[source,sh]
----
oc get pod
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
NAME                                              READY   STATUS    RESTARTS   AGE
capi-provider-5d77d9bd86-g7fzh                    2/2     Running   0          3h39m
catalog-operator-5885cf46f5-lsjzs                 2/2     Running   0          3h37m
certified-operators-catalog-7f67dfdd77-sqgxr      1/1     Running   0          3h37m
cluster-api-545466c99b-brqn5                      1/1     Running   0          3h39m
cluster-autoscaler-69cfc5fc47-8hbmq               1/1     Running   0          3h38m
cluster-network-operator-5d98dfd498-7x964         1/1     Running   0          3h37m
cluster-policy-controller-6bcd98fb5-mflvl         1/1     Running   0          3h37m
cluster-version-operator-795778675-hmdf5          1/1     Running   0          3h37m
community-operators-catalog-74454d659c-zc67b      1/1     Running   0          3h37m
control-plane-operator-649cc79bf4-564jj           2/2     Running   0          3h39m
etcd-0                                            1/1     Running   0          3h38m
hosted-cluster-config-operator-784ff8f67d-bg7n6   1/1     Running   0          3h37m
ignition-server-65847cb4cf-99dsg                  1/1     Running   0          3h38m
ingress-operator-854fbcc9bd-pqrt9                 3/3     Running   0          3h37m
konnectivity-agent-768557489c-zd72d               1/1     Running   0          3h38m
konnectivity-server-5c6c44578f-wrxm6              1/1     Running   0          3h38m
kube-apiserver-6ccf49fc75-4sq2r                   5/5     Running   0          3h38m
kube-controller-manager-5cdd4494d5-t4nkr          2/2     Running   0          3h31m
kube-scheduler-768dd57f48-6z8q5                   1/1     Running   0          3h37m
machine-approver-5b79b89974-8ql98                 1/1     Running   0          3h38m
oauth-openshift-6c66dcb46-9pbll                   2/2     Running   0          3h36m
olm-operator-868bdd99d4-4fsww                     2/2     Running   0          3h37m
openshift-apiserver-5bf7c65998-ncfkr              2/2     Running   0          3h31m
openshift-controller-manager-fcf9f96f7-2q7qp      1/1     Running   0          3h37m
openshift-oauth-apiserver-8674b79648-9k47j        1/1     Running   0          3h37m
ovnkube-master-0                                  6/6     Running   0          3h35m
packageserver-d8996b884-rbssw                     2/2     Running   0          3h37m
redhat-marketplace-catalog-75cdffd7f7-wgtsd       1/1     Running   0          3h37m
redhat-operators-catalog-56566fcdc4-n9mpb         1/1     Running   0          3h37m
----
+
Note all the pods that make up a cluster's control plane: etcd, api servers, various cluster operators, Operator Lifecycle Manager pods etc.

. Examine the `Machines` that were created:
+
[source,sh]
----
oc get machines
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
NAME                                      CLUSTER             NODENAME                                     PROVIDERID                              PHASE     AGE     VERSION
development-us-west-2a-7c8bc8fb8d-k4pg9   development-tm6vf   ip-10-0-129-140.us-west-2.compute.internal   aws:///us-west-2a/i-0b4caf06c96757b88   Running   4h19m   4.11.8
development-us-west-2a-7c8bc8fb8d-nbnmr   development-tm6vf   ip-10-0-138-177.us-west-2.compute.internal   aws:///us-west-2a/i-0b4bbf42f5ff6ba45   Running   4h19m   4.11.8
----
+
Note that these are different `Machine` resources than you are used to. These do not live in the `openshift-machine-api` namespace but in the namespace for the Hypershift cluster. Also they are using a different API, namely the `Machine.cluster.x-k8s.io/v1beta1` API and not the `Machine.machine.openshift.io/v1beta1` API that you may be used to.

. Examine one of those machines (replace the machine name with one from your cluster):
+
[source,sh]
----
oc get machine development-us-west-2a-7c8bc8fb8d-k4pg9 -o yaml
----
+
Note the following properties:

* *OwnerReferences*: These machines are managed by a MachineSet. And just like the Machine that MachineSet is using the `MachineSet.cluster.x-k8s.io/v1beta1` API and not the OpenShift specific API.
* *infrastructureRef*: A reference to an `AWSMachine` - you can examine that object for more details about the provisioned infrastructure.
* *version*: OpenShift Version of the worker node
* *status*: various status items about the Machine including the internal IP address and information about the `Node` that is associated with that Machine

== Examine the provisioned cluster

A *development* cluster has been set up using Hypershift. In order to connect to this cluster you will need a `kubeconfig` file.

The `kubeconfig` configuration is available as a secret in the hub cluster's namespace - in our case this is the `local-cluster` namespace.

. Retrieve the list of secrets in the `local-cluster` namespace:
+
[source,sh]
----
oc get secret -n local-cluster
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
NAME                                               TYPE                                  DATA   AGE
builder-dockercfg-hcssz                            kubernetes.io/dockercfg               1      4h30m
builder-token-fv52v                                kubernetes.io/service-account-token   4      4h30m
default-dockercfg-p42wk                            kubernetes.io/dockercfg               1      4h30m
default-token-ntnlv                                kubernetes.io/service-account-token   4      4h30m
deployer-dockercfg-tlpm5                           kubernetes.io/dockercfg               1      4h30m
deployer-token-95prr                               kubernetes.io/service-account-token   4      4h30m
development-admin-kubeconfig                       Opaque                                1      4h26m
development-kubeadmin-password                     Opaque                                1      4h26m
hypershift-operator-oidc-provider-s3-credentials   Opaque                                3      4h29m
local-cluster-bootstrap-sa-dockercfg-qplb9         kubernetes.io/dockercfg               1      4h30m
local-cluster-bootstrap-sa-token-dksxv             kubernetes.io/service-account-token   4      4h30m
local-cluster-cluster-secret                       Opaque                                3      4h28m
local-cluster-import                               Opaque                                5      4h30m
----

. Get the password for the `kubeadmin` user:
+
[source,sh]
----
oc get secret development-kubeadmin-password -n local-cluster --template='{{ .data.password }}' | base64 -d ; echo
----
+
.Sample Output
[source,texinfo]
----
m6S8T-WReAm-6eqQr-LTyGS
----

. Get the kubeconfig configuration for the `system:admin` user:
+
[source,sh]
----
oc get secret development-admin-kubeconfig -n local-cluster --template='{{ .data.kubeconfig }}' | base64 -d ; echo
----
+
.Sample Output
[source,texinfo]
----
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURFRENDQWZpZ0F3SUJBZ0lJR2ducDhWVEdScGt3RFFZSktvWklodmNOQVFFTEJRQXdKakVTTUJBR0ExVUUKQ3hNSmIzQmxibk5vYVdaME1SQXdEZ1lEVlFRREV3ZHliMjkwTFdOaE1CNFhEVEl5TVRBeU9ERTFOREkxTkZvWApEVE15TVRBeU5URTFOREkxTkZvd0pqRVNNQkFHQTFVRUN4TUpiM0JsYm5Ob2FXWjBNUkF3RGdZRFZRUURFd2R5CmIyOTBMV05oTUlJQklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUEzbVI1LzFmdWRiNlQKSjNueVBJNmphUDNsSzgvYURHV3VhMnMyNmVUNWJ5RnJ6ZW01YzJTS2tvWHMvYzd3N1kzOWVReG43eVFLWmpiNAo1dk1qajM2OEpGdDE4TG5HNnQ0SFRlZUM2YVAzK1pXZ2ZhOUZ2KzcrRDcvdUJQbkxoK2tIK1gycXVXMmlxTktjCm80Y25EV3ZUWnVJT1BSNFo0T0Zxc3VWcEV1OFZMQmZibHRCOTNGVTJTRHEwUzlzemtPb3VuMmhjcjE3ajBsVXAKWnk1enFseGsvU0ZKbklneXhDVEdsZVhOS0ZFOEpYZnVQeXpKQUNFVHZ3U0VRc2tNblBqZkxYaGtDWFNBTXNRYQp3anl6ZGxEVDFhRCszMlRodFFCQ25xbHQ2eGNKM2tXQld4Qnh3M3FSVCtwSEF3MVVEQlg1SGJnbFdMUk81eVlXCnFvelBzRmVPdlFJREFRQUJvMEl3UURBT0JnTlZIUThCQWY4RUJBTUNBcVF3RHdZRFZSMFRBUUgvQkFVd0F3RUIKL3pBZEJnTlZIUTRFRmdRVVZyTUxqalhmZVNzTkZrMDFoSWYxSzdla0Y3NHdEUVlKS29aSWh2Y05BUUVMQlFBRApnZ0VCQUtqQXFPK00zaEFoYml5MlJITWgrdVRQdXcyby83Qjh6eFVZaTBwZjRnMVlrSnV5enBUVC9Rdy9lcGhYClZVNVlYS3NUNW5CeFhGWHNiaGhMNHA3a3had2orU2lBQW1OMzdsbjUxTjFBczI0ZlZZODdhQnFrenZuaEYxdksKQXhiN2hreDdiaDQxOU5Jc0VHVlN6SUlORG8ydlkxWnNJdnJKeHhobW5BUkpja1lxVTBJVytpZUc0MkNreGdMTwprNEV2UzZEOVpyRHdMWlRuR2Juck9Dcis1dmI2ZS9HZVI1OHVnRnRFZzBnN0RJRlVmMUloK216ZzBJY2VCMkxLCll5VjNWZmF3eGRoZjZwS3VYcFlFcTllQmNjU3hLaW1JdHhQeG1TK2cwdU0xL2loUUQ4NTVCQXVqRDZYSTNsYi8KOGdZL3lnRm16dTdsd0hNaFF4WGUxZWlzL2l3PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://ad693fd93879d4d48a3f3086f1af527e-99b9e11c4914ece8.elb.us-east-2.amazonaws.com:6443
  name: cluster

  [... output omitted ...]
----

. Now luckily the environment already has this information available.
+
Set your `KUBECONFIG` environment variable to the kubeconfig file that has been set up on your system:
+
[source,sh]
----
export KUBECONFIG=$HOME/.kube/development.kubeconfig
----

. Now explore the `development` cluster:
+
[source,sh]
----
oc get nodes
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
NAME                                         STATUS   ROLES    AGE     VERSION
ip-10-0-129-140.us-west-2.compute.internal   Ready    worker   4h32m   v1.24.0+dc5a2fd
ip-10-0-138-177.us-west-2.compute.internal   Ready    worker   4h32m   v1.24.0+dc5a2fd
----
+
Note that there are only two `worker` nodes, no `master` nodes. That's because the control plane for this cluster is hosted on the hub cluster as you previously explored.

. Explore the cluster operators:
+
[source,sh]
----
oc get co
----
+
.Sample Output
[source,texinfo,options=nowrap]
----
[lab-user@bastion ~]$ oc get co
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
console                                    4.11.8    True        False         False      4h29m
csi-snapshot-controller                    4.11.8    True        False         False      4h30m
dns                                        4.11.8    True        False         False      4h29m
image-registry                             4.11.8    True        False         False      4h29m
ingress                                    4.11.8    True        False         False      4h29m
insights                                   4.11.8    True        False         False      4h31m
kube-apiserver                             4.11.8    True        False         False      4h36m
kube-controller-manager                    4.11.8    True        False         False      4h36m
kube-scheduler                             4.11.8    True        False         False      4h36m
kube-storage-version-migrator              4.11.8    True        False         False      4h31m
monitoring                                 4.11.8    True        False         False      4h28m
network                                    4.11.8    True        False         False      4h35m
openshift-apiserver                        4.11.8    True        False         False      4h36m
openshift-controller-manager               4.11.8    True        False         False      4h36m
openshift-samples                          4.11.8    True        False         False      4h29m
operator-lifecycle-manager                 4.11.8    True        False         False      4h36m
operator-lifecycle-manager-catalog         4.11.8    True        False         False      4h36m
operator-lifecycle-manager-packageserver   4.11.8    True        False         False      4h36m
service-ca                                 4.11.8    True        False         False      4h31m
storage                                    4.11.8    True        False         False      4h27m
----
+
Note that there a quite a few less cluster operators on a Hypershift cluster than on a regular OpenShift cluster. This is of course because the control plane is not managed by the cluster operators but by Hypershift.

. Get the URL of the OpenShift Console
+
[source,sh]
----
oc whoami --show-console
----
+
.Sample Output
[source,texinfo]
----
https://console-openshift-console.apps.development.rrjmf.sandbox2589.opentlc.com
----

. In a web browser navigate to this URL and log in as `kubeadmin` using the kubeadmin password you retrieved earlier. You can also find this kubeadmin password by running the following command:
+
[source,sh]
----
cat $HOME/.kube/development.kubeadmin-password; echo
----

. Navigate around the managed cluster and notice that it looks just like a regular OpenShift Cluster.

. Close the Console Browser window and switch back to your hub cluster by unsetting the `KUBECONFIG` variable.
+
[source,sh]
----
unset KUBECONFIG
----

== Explore the Hub Cluster Console

The last section in this introductory lab is to examine the hub cluster.

. Retrieve the Console URL (or refer to the e-mail you got when you provisioned the environment).
+
[source,sh]
----
oc whoami -show-console
----
+
.Sample Output
[source,texinfo]
----
https://console-openshift-console.apps.cluster-rrjmf.sandbox2589.opentlc.com
----

. Open this URL in a browser and log into the console using the `admin` user and the password from the welcome e-mail.

. Notice that in the top left corner you see a drop down menu that reads *local-cluster*. This signifies that you are operating on the local cluster - your hub cluster.
. Click the *local-cluster* dropdown and select *All Clusters*. This opens the (simplified version of the) Red Hat Advanced Cluster Management for Kubernetes console.
. If not already there click on *Infrastructure* on the left and then select *Clusters*. You will see that you have two clusters, the *local-cluster* which is also the *Hub* cluster and the *development* cluster that uses *Hypershift* as its infrastructure.
. Click on the *development* cluster and explore the information available. Note that on the *Nodes* tab you also see the two worker nodes that have been provisioned.
 
= Next steps

This concludes the exploratory section of this lab. 

Follow https://github.com/redhat-cop/openshift-lab-origin/blob/master/HyperShift/Deploy_Cluster.adoc[Deploy a cluster using HyperShift] to deploy a new cluster into your environment using HyperShift.
